{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import selenium\n",
    "\n",
    "from src import scrapers\n",
    "\n",
    "# Path to geckodriver executable\n",
    "geckodriver_path = '/snap/bin/firefox.geckodriver'\n",
    "\n",
    "\n",
    "s = Service(executable_path=geckodriver_path)\n",
    "# Create a Firefox webdriver instance\n",
    "# opens a window\n",
    "driver = webdriver.Firefox(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UolNewsScraper():\n",
    "    \"\"\"Scraper for the G1 news website\"\"\"\n",
    "\n",
    "    def __init__(self, n_scrolls = 10):\n",
    "        self.url = 'https://noticias.uol.com.br/?clv3=true'\n",
    "        \n",
    "        # how many times to scrool the page\n",
    "        self.n_scrolls = n_scrolls\n",
    "        \n",
    "    def _scroll_page(self, driver: selenium.webdriver):\n",
    "        \"\"\"Scroll the page by n_scrolls iterations\"\"\"\n",
    "        \n",
    "        current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        for i in range(self.n_scrolls):\n",
    "            # scroll to the end of the page\n",
    "            driver.execute_script(f\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            sleep(1)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            \n",
    "            # if not autoload\n",
    "            if new_height == current_height:\n",
    "                # click the 'Veja mais' button\n",
    "                driver.find_element(By.CSS_SELECTOR, value='.btn-search').click()\n",
    "                \n",
    "                # repeat the scroll\n",
    "                driver.execute_script(f\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "                current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            else:\n",
    "                current_height = new_height\n",
    "                \n",
    "            print(f'Scroll {i + 1}, height={current_height}')\n",
    "            \n",
    "            # time for the page to load\n",
    "            sleep(4)\n",
    "            \n",
    "        \n",
    "    def _get_scraped_news(self, driver: selenium.webdriver):\n",
    "        \"\"\"Obtain the scraped news from the website\"\"\"\n",
    "        html_source = driver.page_source\n",
    "        soup = BeautifulSoup(html_source, 'lxml')\n",
    "        \n",
    "        content = soup.find('section', class_ = 'latest-news')\n",
    "        news_list = content.find_all('div', class_ = 'thumb-caption')\n",
    "        \n",
    "        \n",
    "        titles = []\n",
    "        times = []\n",
    "        resumes = []\n",
    "\n",
    "        for news in news_list:\n",
    "            # obtain the info of a individual news\n",
    "            title = news.find('h3', class_ = 'thumb-title')\n",
    "            titles.append(title.text)\n",
    "\n",
    "            time = news.find('time', class_ = 'thumb-date')\n",
    "            times.append(None if time is None else time.text)\n",
    "\n",
    "            resume = news.find('p', class_ = 'thumb-description')\n",
    "            resumes.append(None if resume is None else resume.text)\n",
    "\n",
    "        # create the final dataframe\n",
    "        news_df = pd.DataFrame({\n",
    "            'Title': titles,\n",
    "            'Time': times,\n",
    "            'Resume': resumes\n",
    "        })     \n",
    "\n",
    "        # create the final dataframe\n",
    "        news_df = pd.DataFrame({\n",
    "            'Title': titles,\n",
    "            'Time': times,\n",
    "            'Resume': resumes\n",
    "        })     \n",
    "        \n",
    "        return news_df\n",
    "    \n",
    "    def _convert_to_datetime(self, time_str: str) -> datetime:\n",
    "        return datetime.strptime(time_str, '%d/%m/%Y %Hh%M')\n",
    "        \n",
    "    \n",
    "    def _data_cleaning(self, news_df: pd.DataFrame):\n",
    "        \"\"\"Do simple data cleaning after the scrap\"\"\"\n",
    "        news_df['Time'] = news_df['Time'].apply(self._convert_to_datetime)\n",
    "        return news_df\n",
    "\n",
    "\n",
    "    def scrap_news(self, driver: selenium.webdriver) -> pd.DataFrame:\n",
    "        \"\"\"Scrap the news for the G1 website\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        driver : selenium.webdriver\n",
    "            Current webdriver\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            The news scraped\n",
    "        \"\"\"\n",
    "        driver.get(self.url)\n",
    "        sleep(1)\n",
    "        \n",
    "        self._scroll_page(driver)\n",
    "        \n",
    "        news_df = self._get_scraped_news(driver)\n",
    "        \n",
    "        news_df = self._data_cleaning(news_df)\n",
    "        \n",
    "        return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = UolNewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scroll 1, height=9945\n",
      "Scroll 2, height=11144\n",
      "Scroll 3, height=12597\n",
      "Scroll 4, height=13730\n",
      "Scroll 5, height=13730\n",
      "Scroll 6, height=14885\n",
      "Scroll 7, height=16188\n",
      "Scroll 8, height=17160\n",
      "Scroll 9, height=17225\n",
      "Scroll 10, height=17225\n"
     ]
    }
   ],
   "source": [
    "news_df = scraper.scrap_news(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title             object\n",
       "Time      datetime64[ns]\n",
       "Resume            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
